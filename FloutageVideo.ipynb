{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c0e659",
   "metadata": {},
   "source": [
    "# üé• Floutage Vid√©o Intelligent d‚ÄôObjets Sensibles\n",
    "\n",
    "Ce notebook impl√©mente exactement la **m√™me logique de traitement vid√©o**\n",
    "que l‚Äôapplication **Streamlit** associ√©e.\n",
    "\n",
    "Fonctionnalit√©s :\n",
    "- D√©tection YOLOv8 (visages, plaques, √©crans)\n",
    "- Tracking persistant avec **KCF**\n",
    "- Floutage ajustable\n",
    "- Traitement frame par frame\n",
    "\n",
    "‚ö†Ô∏è Ce notebook partage **les m√™mes fonctions, variables et logique**\n",
    "que l‚Äôapplication Streamlit (hors interface).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61989c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Video, display\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1b4e3",
   "metadata": {},
   "source": [
    "## üîπ Chargement des mod√®les YOLO\n",
    "\n",
    "Trois mod√®les sont utilis√©s :\n",
    "- **Visages** : mod√®le YOLO sp√©cialis√©\n",
    "- **Plaques** : mod√®le ALPR entra√Æn√©\n",
    "- **Objets COCO** : mod√®le YOLOv8 standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b45def9-f0ab-4e4e-a9e8-033e6113b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_modeles():\n",
    "    return {\n",
    "        \"face\": YOLO(\"yolov8s-face-lindevs.pt\"),\n",
    "        \"alpr\": YOLO(\"best.pt\"),\n",
    "        \"coco\": YOLO(\"yolov8n.pt\")\n",
    "    }\n",
    "\n",
    "models = charger_modeles()\n",
    "OBJETS_COCO = [\"laptop\", \"cell phone\", \"tv\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0a4b1",
   "metadata": {},
   "source": [
    "## üîπ Fonctions utilitaires\n",
    "\n",
    "Ces fonctions permettent :\n",
    "- De s√©curiser les bounding boxes\n",
    "- D‚Äôappliquer un floutage rectangulaire ou elliptique\n",
    "- D‚Äôafficher un cercle de progression (facultatif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bb01410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_bbox(bbox, shape):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    x1 = max(0, x1); y1 = max(0, y1)\n",
    "    x2 = min(shape[1]-1, x2); y2 = min(shape[0]-1, y2)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def flouter_roi(frame, bbox, intensite, ellipse=False):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return\n",
    "\n",
    "    k = max(15, ((x2-x1)//3)|1) * intensite\n",
    "    flou = cv2.GaussianBlur(roi, (k, k), 0)\n",
    "\n",
    "    if ellipse:\n",
    "        mask = np.zeros(roi.shape[:2], dtype=np.uint8)\n",
    "        cx, cy = (x2-x1)//2, (y2-y1)//2\n",
    "        rx = int((x2-x1) * 0.95 / 2)\n",
    "        ry = int((y2-y1) * 0.95 / 2)\n",
    "        cv2.ellipse(mask, (cx, cy), (rx, ry), 0, 0, 360, 255, -1)\n",
    "        roi[mask == 255] = flou[mask == 255]\n",
    "    else:\n",
    "        roi[:] = flou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6121df60",
   "metadata": {},
   "source": [
    "## üîπ Param√®tres globaux\n",
    "\n",
    "- `FRAME_STEP` : fr√©quence de d√©tection YOLO\n",
    "- `MAX_MISSED` : nombre max de frames perdues avant suppression du tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc60aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_STEP = 1\n",
    "MAX_MISSED = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891828fc",
   "metadata": {},
   "source": [
    "## üîπ Traitement principal de la vid√©o\n",
    "\n",
    "Cette fonction :\n",
    "- Charge la vid√©o upload√©e\n",
    "- Applique la d√©tection + tracking\n",
    "- Floute dynamiquement les objets\n",
    "- Sauvegarde la vid√©o finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9958dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traiter_video(video_bytes, use_face, use_alpr, use_coco, intensite_flou):\n",
    "    temp_in = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
    "    temp_in.write(video_bytes)\n",
    "    temp_in.close()\n",
    "\n",
    "    cap = cv2.VideoCapture(temp_in.name)\n",
    "    w, h = int(cap.get(3)), int(cap.get(4))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Chemin exact que tu m'as donn√©\n",
    "    out_path = \"C:/Users/fatim/OneDrive/Bureau/Vid√©os/video_flout√©e.mp4\"\n",
    "    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    trackers = {\"face\": [], \"alpr\": [], \"coco\": []}\n",
    "    last_bboxes = {\"face\": [], \"alpr\": [], \"coco\": []}\n",
    "    missed = {\"face\": [], \"alpr\": [], \"coco\": []}\n",
    "    frame_id = 0\n",
    "\n",
    "    print(\"D√©but du traitement vid√©o...\")\n",
    "\n",
    "    for _ in tqdm(range(total), desc=\"Traitement des frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_id += 1\n",
    "\n",
    "        # D√©tection YOLO\n",
    "        if frame_id % FRAME_STEP == 0:\n",
    "            small = cv2.resize(frame, (640, int(640*h/w)))\n",
    "            for key, enabled, labels in [\n",
    "                (\"face\", use_face, None),\n",
    "                (\"alpr\", use_alpr, None),\n",
    "                (\"coco\", use_coco, OBJETS_COCO)\n",
    "            ]:\n",
    "                if not enabled:\n",
    "                    continue\n",
    "                result = models[key](small, conf=0.4, verbose=False)[0]\n",
    "                trackers[key], last_bboxes[key], missed[key] = [], [], []\n",
    "                for box, cls in zip(result.boxes.xyxy, result.boxes.cls):\n",
    "                    if labels and models[key].names[int(cls)] not in labels:\n",
    "                        continue\n",
    "                    bbox = [\n",
    "                        box[0]*w/640, box[1]*h/small.shape[0],\n",
    "                        box[2]*w/640, box[3]*h/small.shape[0]\n",
    "                    ]\n",
    "                    safe = clamp_bbox(bbox, frame.shape)\n",
    "                    if safe:\n",
    "                        flouter_roi(frame, safe, intensite_flou, ellipse=(key==\"face\"))\n",
    "                        tracker = cv2.TrackerCSRT_create()\n",
    "                        x1,y1,x2,y2 = safe\n",
    "                        tracker.init(frame, (x1,y1,x2-x1,y2-y1))\n",
    "                        trackers[key].append(tracker)\n",
    "                        last_bboxes[key].append(safe)\n",
    "                        missed[key].append(0)\n",
    "\n",
    "        # Tracking persistant\n",
    "        else:\n",
    "            for key in trackers:\n",
    "                new_trackers = []\n",
    "                new_bboxes = []\n",
    "                new_missed = []\n",
    "                for i, tr in enumerate(trackers[key]):\n",
    "                    ok, b = tr.update(frame)\n",
    "                    if ok:\n",
    "                        x, y, w0, h0 = map(int, b)\n",
    "                        safe = clamp_bbox((x, y, x+w0, y+h0), frame.shape)\n",
    "                        if safe:\n",
    "                            flouter_roi(frame, safe, intensite_flou, ellipse=(key==\"face\"))\n",
    "                            new_trackers.append(tr)\n",
    "                            new_bboxes.append(safe)\n",
    "                            new_missed.append(0)\n",
    "                    else:\n",
    "                        if i < len(last_bboxes[key]) and missed[key][i] < MAX_MISSED:\n",
    "                            safe = last_bboxes[key][i]\n",
    "                            flouter_roi(frame, safe, intensite_flou, ellipse=(key==\"face\"))\n",
    "                            new_trackers.append(tr)\n",
    "                            new_bboxes.append(safe)\n",
    "                            new_missed.append(missed[key][i]+1)\n",
    "                trackers[key] = new_trackers\n",
    "                last_bboxes[key] = new_bboxes\n",
    "                missed[key] = new_missed\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"‚úÖ Traitement termin√©. Vid√©o sauvegard√©e dans : {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeba02c",
   "metadata": {},
   "source": [
    "## üîπ Interface interactive\n",
    "\n",
    "L‚Äôutilisateur peut :\n",
    "- Uploader une vid√©o\n",
    "- Choisir les types de floutage\n",
    "- R√©gler l‚Äôintensit√© du flou\n",
    "- Lancer le traitement en un clic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe8d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f99996dace405c8efb83a7a0f9a9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.mp4,.avi,.mov', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1321940ab05e4b5697a410cc395ef94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Flouter les visages')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f48224efc441f983ef34a0344519d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Flouter les plaques')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e437feaba249cab05fafcd311caca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Flouter les √©crans')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d49126738a4eab8937e641532d81e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=3, description='Intensit√© du flou', max=5, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148817e8db0c4cccba8de92deefcdb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Lancer le floutage', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a7ca9c17c4437f921f4643983350e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_widget = widgets.FileUpload(accept=\".mp4,.avi,.mov\", multiple=False)\n",
    "face_checkbox = widgets.Checkbox(value=True, description=\"Flouter les visages\")\n",
    "alpr_checkbox = widgets.Checkbox(value=True, description=\"Flouter les plaques\")\n",
    "coco_checkbox = widgets.Checkbox(value=True, description=\"Flouter les √©crans\")\n",
    "intensite_slider = widgets.IntSlider(value=3, min=1, max=5, description=\"Intensit√© du flou\")\n",
    "process_button = widgets.Button(description=\"Lancer le floutage\")\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "display(upload_widget)\n",
    "display(face_checkbox, alpr_checkbox, coco_checkbox, intensite_slider)\n",
    "display(process_button, output_widget)\n",
    "\n",
    "def get_uploaded_file(widget):\n",
    "    if not widget.value:\n",
    "        return None\n",
    "    return widget.value[0][\"content\"] if isinstance(widget.value, tuple) \\\n",
    "        else list(widget.value.values())[0][\"content\"]\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()\n",
    "\n",
    "        video_bytes = get_uploaded_file(upload_widget)\n",
    "        if video_bytes is None:\n",
    "            print(\"‚ö†Ô∏è Veuillez uploader une vid√©o.\")\n",
    "            return\n",
    "\n",
    "        out_path = traiter_video(\n",
    "            video_bytes,\n",
    "            face_checkbox.value,\n",
    "            alpr_checkbox.value,\n",
    "            coco_checkbox.value,\n",
    "            intensite_slider.value\n",
    "        )\n",
    "\n",
    "\n",
    "process_button.on_click(on_button_click)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
