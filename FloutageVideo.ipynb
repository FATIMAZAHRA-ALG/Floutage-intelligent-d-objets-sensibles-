{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdcf460a-440e-4984-964c-01584d66e08b",
   "metadata": {},
   "source": [
    "# üé• Floutage automatique de vid√©o par vision par ordinateur\n",
    "\n",
    "## Objectif du projet\n",
    "Ce projet vise √† prot√©ger la vie priv√©e dans les vid√©os num√©riques en\n",
    "d√©tectant et en floutant automatiquement des √©l√©ments sensibles tels que :\n",
    "- les visages,\n",
    "- les plaques d‚Äôimmatriculation,\n",
    "- les √©crans (ordinateur, t√©l√©phone, t√©l√©vision).\n",
    "\n",
    "Le traitement repose sur des techniques de vision par ordinateur et\n",
    "d‚Äôapprentissage profond appliqu√©es image par image sur une s√©quence vid√©o.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289a413-ffdd-48de-8db1-5606f631492a",
   "metadata": {},
   "source": [
    "### Importation des biblioth√®ques\n",
    "\n",
    "Cette cellule permet d‚Äôimporter les biblioth√®ques n√©cessaires au traitement vid√©o et √† la d√©tection d‚Äôobjets :\n",
    "\n",
    "- **OpenCV (cv2)** : utilis√©e pour la lecture, le traitement et la manipulation des images et des vid√©os.\n",
    "- **NumPy** : permet de g√©rer efficacement les tableaux et les calculs num√©riques.\n",
    "- **YOLO (Ultralytics)** : mod√®le de d√©tection d‚Äôobjets bas√© sur le deep learning, utilis√© pour identifier automatiquement les objets sensibles.\n",
    "- **Matplotlib** : utilis√©e pour l‚Äôaffichage et la visualisation des images.\n",
    "- **IPython.display** : permet d‚Äôafficher directement des vid√©os dans le notebook Jupyter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3782bb51-d03e-4cea-b2a6-b69dc28e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, display\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088c06e-123f-4cf2-a247-ad7724524401",
   "metadata": {},
   "source": [
    "### Chargement des mod√®les de d√©tection\n",
    "\n",
    "Cette cellule initialise les mod√®les de d√©tection d‚Äôobjets bas√©s sur YOLO, chacun √©tant sp√©cialis√© dans un type d‚Äô√©l√©ments sensibles :\n",
    "\n",
    "- **Mod√®le de d√©tection des visages** : utilis√© pour identifier automatiquement les visages pr√©sents dans la vid√©o.\n",
    "- **Mod√®le ALPR (Automatic License Plate Recognition)** : permet de d√©tecter les plaques d‚Äôimmatriculation afin de pr√©server la confidentialit√©.\n",
    "- **Mod√®le COCO** : mod√®le g√©n√©raliste entra√Æn√© sur le jeu de donn√©es COCO, utilis√© ici pour d√©tecter certains objets sensibles.\n",
    "\n",
    "La liste `COCO_TARGETS` d√©finit les cat√©gories d‚Äôobjets √† flouter, telles que les ordinateurs portables, les t√©l√©phones portables et les √©crans de t√©l√©vision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c78be8-4b08-4839-9f59-72944ed49b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model = YOLO(\"yolov8s-face-lindevs.pt\")\n",
    "alpr_model = YOLO(\"best.pt\")\n",
    "coco_model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "COCO_TARGETS = [\"laptop\", \"cell phone\", \"tv\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c6288-8bd9-4de1-8fc1-6112709f9d17",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires pour le floutage\n",
    "\n",
    "Cette cellule d√©finit deux fonctions essentielles utilis√©es lors du traitement vid√©o :\n",
    "\n",
    "- **`clamp_bbox`** :  \n",
    "  Cette fonction ajuste les coordonn√©es d‚Äôune bo√Æte englobante afin qu‚Äôelles restent √† l‚Äôint√©rieur des dimensions de l‚Äôimage. Elle permet d‚Äô√©viter les erreurs li√©es aux d√©bordements et ignore les r√©gions invalides.\n",
    "\n",
    "- **`flouter_roi`** :  \n",
    "  Cette fonction applique un floutage gaussien sur une r√©gion d‚Äôint√©r√™t (ROI) correspondant √† un objet d√©tect√©.  \n",
    "  L‚Äôintensit√© du floutage est automatiquement adapt√©e √† la taille de la r√©gion, et deux modes sont possibles :\n",
    "  - floutage rectangulaire classique,\n",
    "  - floutage elliptique, particuli√®rement adapt√© au floutage des visages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5d6cc5-9219-457f-8d8d-4f4134478178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_bbox(bbox, shape):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    x1 = max(0, x1)\n",
    "    y1 = max(0, y1)\n",
    "    x2 = min(shape[1]-1, x2)\n",
    "    y2 = min(shape[0]-1, y2)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def flouter_roi(frame, bbox, intensite=3, ellipse=False):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return\n",
    "\n",
    "    k = max(15, ((x2-x1)//3) | 1) * intensite\n",
    "    flou = cv2.GaussianBlur(roi, (k, k), 0)\n",
    "\n",
    "    if ellipse:\n",
    "        mask = np.zeros(roi.shape[:2], dtype=np.uint8)\n",
    "        cv2.ellipse(\n",
    "            mask,\n",
    "            ((x2-x1)//2, (y2-y1)//2),\n",
    "            ((x2-x1)//2, (y2-y1)//2),\n",
    "            0, 0, 360, 255, -1\n",
    "        )\n",
    "        roi[mask == 255] = flou[mask == 255]\n",
    "    else:\n",
    "        roi[:] = flou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99c0bf-2023-43fe-82f6-805f670be9bf",
   "metadata": {},
   "source": [
    "### Chargement et configuration de la vid√©o\n",
    "\n",
    "Cette cellule permet d‚Äôouvrir la vid√©o d‚Äôentr√©e et d‚Äôextraire ses principales propri√©t√©s, telles que la largeur, la hauteur et le nombre d‚Äôimages par seconde (FPS).\n",
    "\n",
    "Un objet `VideoWriter` est ensuite initialis√© afin d‚Äôenregistrer la vid√©o de sortie apr√®s application du floutage des √©l√©ments sensibles, tout en conservant les caract√©ristiques originales de la vid√©o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f0c139-9b02-4bc6-aae4-f248b9fbd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"C:\\\\Users\\\\fatim\\\\OneDrive\\\\Bureau\\\\Voiture.mp4\"  # chemin vers la vid√©o\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "out = cv2.VideoWriter(\n",
    "    \"output_floute.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps,\n",
    "    (width, height)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b45f8-79ba-4459-9590-2ffbdb8dab61",
   "metadata": {},
   "source": [
    "### Traitement des images vid√©o et application du floutage\n",
    "\n",
    "Cette cellule r√©alise le traitement principal de la vid√©o image par image.  \n",
    "Chaque frame est redimensionn√©e afin d‚Äôacc√©l√©rer la d√©tection des objets par les mod√®les YOLO.\n",
    "\n",
    "Les mod√®les de d√©tection (visages, plaques d‚Äôimmatriculation et objets COCO cibl√©s) sont ensuite appliqu√©s successivement.  \n",
    "Pour chaque objet d√©tect√©, les coordonn√©es sont recalcul√©es √† l‚Äô√©chelle originale de la vid√©o, puis v√©rifi√©es avant d‚Äôappliquer un floutage adapt√© √† la r√©gion concern√©e.\n",
    "\n",
    "Les frames modifi√©es sont enfin enregistr√©es dans une nouvelle vid√©o de sortie, garantissant la protection des √©l√©ments sensibles tout en conservant la qualit√© globale de la sc√®ne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c8aec3-0380-4282-8218-757eb82d040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_id = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "    small = cv2.resize(frame, (640, int(640 * height / width)))\n",
    "\n",
    "    for model, ellipse, labels in [\n",
    "        (face_model, True, None),\n",
    "        (alpr_model, False, None),\n",
    "        (coco_model, False, COCO_TARGETS)\n",
    "    ]:\n",
    "        results = model(small, conf=0.4, verbose=False)[0]\n",
    "\n",
    "        for box, cls in zip(results.boxes.xyxy, results.boxes.cls):\n",
    "            if labels and model.names[int(cls)] not in labels:\n",
    "                continue\n",
    "\n",
    "            bbox = [\n",
    "                box[0] * width / 640,\n",
    "                box[1] * height / small.shape[0],\n",
    "                box[2] * width / 640,\n",
    "                box[3] * height / small.shape[0]\n",
    "            ]\n",
    "\n",
    "            safe = clamp_bbox(bbox, frame.shape)\n",
    "            if safe:\n",
    "                flouter_roi(frame, safe, intensite=3, ellipse=ellipse)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474d0b1-5569-442d-9ffe-96ede2a0f211",
   "metadata": {},
   "source": [
    "### Traitement des images vid√©o et application du floutage\n",
    "\n",
    "Cette cellule r√©alise le traitement principal de la vid√©o image par image.  \n",
    "Chaque frame est redimensionn√©e afin d‚Äôacc√©l√©rer la d√©tection des objets par les mod√®les YOLO.\n",
    "\n",
    "Les mod√®les de d√©tection (visages, plaques d‚Äôimmatriculation et objets COCO cibl√©s) sont ensuite appliqu√©s successivement.  \n",
    "Pour chaque objet d√©tect√©, les coordonn√©es sont recalcul√©es √† l‚Äô√©chelle originale de la vid√©o, puis v√©rifi√©es avant d‚Äôappliquer un floutage adapt√© √† la r√©gion concern√©e.\n",
    "\n",
    "Les frames modifi√©es sont enfin enregistr√©es dans une nouvelle vid√©o de sortie, garantissant la protection des √©l√©ments sensibles tout en conservant la qualit√© globale de la sc√®ne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31a35cce-f248-4593-bfbe-91125d1e36cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚úÖ La vid√©o a √©t√© enregistr√©e avec succ√®s : `C:\\Users\\fatim\\OneDrive\\Bureau\\Projects_S3\\TraitementVideo\\output_floute.mp4`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "video_path = r\"C:\\Users\\fatim\\OneDrive\\Bureau\\Projects_S3\\TraitementVideo\\output_floute.mp4\"\n",
    "\n",
    "# Message simple\n",
    "display(Markdown(f\"‚úÖ La vid√©o a √©t√© enregistr√©e avec succ√®s : `{video_path}`\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7ce58-c2d1-4b90-b641-e7485b3f829d",
   "metadata": {},
   "source": [
    "## Discussion et pistes d‚Äôam√©lioration\n",
    "\n",
    "- Acc√©l√©ration du traitement √† l‚Äôaide du GPU\n",
    "- Utilisation d‚Äôun tracker multi-objets plus robuste\n",
    "- Am√©lioration de la d√©tection des plaques lointaines\n",
    "- Passage √† un traitement en temps r√©el\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8fb9e-c7b5-4b56-a7be-6cf87eb04c02",
   "metadata": {},
   "source": [
    "## Remarque compl√©mentaire\n",
    "\n",
    "En compl√©ment de ce notebook Jupyter, une application interactive a √©t√©\n",
    "d√©velopp√©e √† l‚Äôaide de Streamlit.  \n",
    "Cette application repose sur le m√™me pipeline de traitement vid√©o et permet\n",
    "une utilisation plus intuitive (import de vid√©o, choix des options de floutage,\n",
    "visualisation avant/apr√®s).\n",
    "\n",
    "Lien vers l‚Äôapplication Streamlit :  \n",
    "üëâ https://lien-de-ton-app.streamlit.app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364efafd-4a46-43f1-a12e-2381a229f81b",
   "metadata": {},
   "source": [
    "### Conclusion technique\n",
    "\n",
    "Ce notebook a permis de mettre en ≈ìuvre une application compl√®te de traitement vid√©o bas√©e sur la vision par ordinateur et le deep learning. En s‚Äôappuyant sur plusieurs mod√®les YOLO sp√©cialis√©s, le syst√®me est capable de d√©tecter automatiquement diff√©rents √©l√©ments sensibles tels que les visages, les plaques d‚Äôimmatriculation et certains objets du quotidien.\n",
    "\n",
    "Le traitement image par image, combin√© √† un redimensionnement optimis√© et √† une gestion s√©curis√©e des r√©gions d‚Äôint√©r√™t, garantit un bon compromis entre pr√©cision de d√©tection et performance d‚Äôex√©cution. L‚Äôapplication du floutage gaussien, ajust√© dynamiquement √† la taille des objets d√©tect√©s, assure une protection efficace de la vie priv√©e tout en pr√©servant la lisibilit√© globale de la vid√©o.\n",
    "\n",
    "Cette approche modulaire et extensible constitue une base solide pour des applications r√©elles telles que l‚Äôanonymisation de vid√©os, l‚Äôobservation intelligente ou les syst√®mes de s√©curit√© respectueux de la confidentialit√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd4d06-1e69-4e27-98bb-e1e71b0cb86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
